{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:56:37.527341Z","iopub.status.busy":"2023-06-22T12:56:37.526857Z","iopub.status.idle":"2023-06-22T12:57:09.499382Z","shell.execute_reply":"2023-06-22T12:57:09.498122Z","shell.execute_reply.started":"2023-06-22T12:56:37.527305Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","!wget https://dl.google.com/linux/linux_signing_key.pub\n","!sudo apt-key add linux_signing_key.pub\n","!echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' >> /etc/apt/sources.list.d/google-chrome.list\n","!sudo apt-get -y update\n","!sudo apt-get install -y google-chrome-stable"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:57:09.502647Z","iopub.status.busy":"2023-06-22T12:57:09.502245Z","iopub.status.idle":"2023-06-22T12:57:12.026449Z","shell.execute_reply":"2023-06-22T12:57:12.024854Z","shell.execute_reply.started":"2023-06-22T12:57:09.502608Z"},"trusted":true},"outputs":[],"source":["# install chromedriver\n","# !apt-get install -y qq unzip\n","!wget -O /tmp/chromedriver.zip http://chromedriver.storage.googleapis.com/`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip\n","!unzip /tmp/chromedriver.zip chromedriver -d /usr/local/bin/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:57:12.029058Z","iopub.status.busy":"2023-06-22T12:57:12.028336Z","iopub.status.idle":"2023-06-22T12:57:57.992210Z","shell.execute_reply":"2023-06-22T12:57:57.991037Z","shell.execute_reply.started":"2023-06-22T12:57:12.029019Z"},"trusted":true},"outputs":[],"source":["# install selenium\n","!sudo apt install -y python3-selenium\n","!pip install selenium scrapy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:57:57.998066Z","iopub.status.busy":"2023-06-22T12:57:57.997594Z","iopub.status.idle":"2023-06-22T12:57:59.246704Z","shell.execute_reply":"2023-06-22T12:57:59.244983Z","shell.execute_reply.started":"2023-06-22T12:57:57.998025Z"},"trusted":true},"outputs":[],"source":["# To check Google Chrome's version\n","!google-chrome --version"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:57:59.248944Z","iopub.status.busy":"2023-06-22T12:57:59.248535Z","iopub.status.idle":"2023-06-22T12:58:00.347703Z","shell.execute_reply":"2023-06-22T12:58:00.346371Z","shell.execute_reply.started":"2023-06-22T12:57:59.248905Z"},"trusted":true},"outputs":[],"source":["# To check Chrome Driver's version\n","!chromedriver -v"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:00.350054Z","iopub.status.busy":"2023-06-22T12:58:00.349640Z","iopub.status.idle":"2023-06-22T12:58:00.396205Z","shell.execute_reply":"2023-06-22T12:58:00.394963Z","shell.execute_reply.started":"2023-06-22T12:58:00.350013Z"},"trusted":true},"outputs":[],"source":["from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:00.398091Z","iopub.status.busy":"2023-06-22T12:58:00.397738Z","iopub.status.idle":"2023-06-22T12:58:00.405444Z","shell.execute_reply":"2023-06-22T12:58:00.404111Z","shell.execute_reply.started":"2023-06-22T12:58:00.398063Z"},"trusted":true},"outputs":[],"source":["import time"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Scrape jobs.cz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:00.407500Z","iopub.status.busy":"2023-06-22T12:58:00.407125Z","iopub.status.idle":"2023-06-22T12:58:00.417511Z","shell.execute_reply":"2023-06-22T12:58:00.416242Z","shell.execute_reply.started":"2023-06-22T12:58:00.407468Z"},"trusted":true},"outputs":[],"source":["def jobscz_selenium(url):\n","    chrome_options = webdriver.ChromeOptions()\n","    chrome_options.add_argument('--no-sandbox')\n","    chrome_options.add_argument('--headless')\n","    chrome_options.add_argument('--disable-gpu')\n","    chrome_options.add_argument('--disable-dev-shm-usage')\n","    chrome_options.add_argument(\"--window-size=1920,1080\")\n","\n","    driver = webdriver.Chrome(\n","        options=chrome_options\n","    )\n","    driver.get(url)\n","    \"\"\"wait = WebDriverWait(driver, 20)  # Maximum wait time of 10 seconds\n","    div_element = wait.until(\n","        EC.visibility_of_element_located((By.CLASS_NAME, \"cp-detail__content\"))\n","    )\"\"\"\n","    time.sleep(5)\n","    div_element = driver.find_element(By.CLASS_NAME, \"cp-detail__content\")\n","    text_values = div_element.text\n","    driver.quit()\n","    return text_values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:00.419368Z","iopub.status.busy":"2023-06-22T12:58:00.418994Z","iopub.status.idle":"2023-06-22T12:58:01.303582Z","shell.execute_reply":"2023-06-22T12:58:01.301739Z","shell.execute_reply.started":"2023-06-22T12:58:00.419338Z"},"trusted":true},"outputs":[],"source":["import datetime\n","import re\n","import os\n","import scrapy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:01.309907Z","iopub.status.busy":"2023-06-22T12:58:01.309066Z","iopub.status.idle":"2023-06-22T12:58:01.328282Z","shell.execute_reply":"2023-06-22T12:58:01.326874Z","shell.execute_reply.started":"2023-06-22T12:58:01.309857Z"},"trusted":true},"outputs":[],"source":["class DataSpider(scrapy.Spider):\n","    name = \"data\"\n","    allowed_domains = [\"jobs.cz\"]\n","    start_urls = [\n","        \"https://www.jobs.cz/prace/?q%5B%5D=machine%20learning&q%5B%5D=data%20analytics&q%5B%5D=data%20science&q%5B%5D=MLOps&q%5B%5D=ML&q%5B%5D=strojové%20učení&q%5B%5D=BI%20analytik&q%5B%5D=Buissness%20Analytik&q%5B%5D=bi%20analyst&q%5B%5D=data%20analyst&profession%5B%5D=201100610&profession%5B%5D=201100592&profession%5B%5D=201100418&profession%5B%5D=201100618&profession%5B%5D=201100658\"\n","    ]\n","            \n","    def parse(self, response):\n","        jobs = response.css(\"article.SearchResultCard\")\n","\n","        for job in jobs:\n","            relative_url = job.css(\"h2 a ::attr(href)\").get()\n","            title = job.css(\"h2 a ::text\").get().strip()\n","            pay_range = job.css(\"span[class*=Tag--success]::text\").get()\n","            comapny = response.css(\n","                \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:first-child span::text\"\n","            ).get()\n","            location = (\n","                response.css(\n","                    \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:nth-child(2)\"\n","                )\n","                .re(r\">([^<]+)<\")[3]\n","                .strip()\n","            )\n","            yield response.follow(\n","                relative_url,\n","                callback=self.parse_inzerat,\n","                meta={\n","                    \"title\": title,\n","                    \"pay_range\": pay_range,\n","                    \"comapny\": comapny,\n","                    \"location\": location,\n","                },\n","            )\n","\n","        \"\"\"next_page = response.css(\n","            \"ul.Pagination li.Pagination__item:last-child a::attr(href)\"\n","        ).get()\n","        if next_page:\n","            next_page_url = self.domain_name + next_page\n","            yield response.follow(next_page_url, callback=self.parse)\"\"\"\n","\n","    def parse_inzerat(self, response):\n","        all_text = \"Not Found\"\n","        div_content = (\n","            response.css(\"section.cp-detail__content\")\n","            or response.css(\"div.cp-detail__content\")\n","            or response.css(\"div.content-rich-text\")\n","            or response.css(\"div.standalone.jobad__body\")\n","        )\n","        if div_content:\n","            # Extract text from <p> tags\n","            paragraphs = div_content.css(\"p::text\").getall()\n","\n","            # Extract text from <h2> tags\n","            headings = div_content.css(\"h2::text\").getall()\n","\n","            # Extract text from <ul> tags\n","            list_items = div_content.css(\"ul li::text\").getall()\n","\n","            # Extract text from <strong> tags\n","            strong_text = div_content.css(\"strong::text\").getall()\n","            \n","            i_text = div_content.css(\"i::text\").getall()\n","            \n","            u_text = div_content.css(\"u::text\").getall()\n","            # Join all the extracted text into a single string\n","            all_text = div_content.select(\"text()\").extract()\n","        else:\n","            #all_text = jobscz_selenium(response.url)\n","            pass\n","\n","        yield {\"url\": response.url, \"text\": all_text, **response.meta, \"lists\": list_items,\n","               \"paragraphs\": paragraphs, strong_text: }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:01.331212Z","iopub.status.busy":"2023-06-22T12:58:01.330776Z","iopub.status.idle":"2023-06-22T12:58:01.349306Z","shell.execute_reply":"2023-06-22T12:58:01.347964Z","shell.execute_reply.started":"2023-06-22T12:58:01.331178Z"},"trusted":true},"outputs":[],"source":["class WebDevSpider(scrapy.Spider):\n","    name = \"webdev\"\n","    allowed_domains = [\"jobs.cz\"]\n","    start_urls = [\n","        \"https://www.jobs.cz/prace/?q%5B%5D=frontend&q%5B%5D=backend&q%5B%5D=full%20stack&q%5B%5D=web%20developer&q%5B%5D=react&q%5B%5D=vue&q%5B%5D=java&q%5B%5D=javascript&q%5B%5D=typescript&q%5B%5D=spring&q%5B%5D=django&q%5B%5D=flask&q%5B%5D=node&q%5B%5D=angular&page=2\"\n","    ]\n","            \n","    def parse(self, response):\n","        jobs = response.css(\"article.SearchResultCard\")\n","\n","        for job in jobs:\n","            relative_url = job.css(\"h2 a ::attr(href)\").get()\n","            title = job.css(\"h2 a ::text\").get().strip()\n","            pay_range = job.css(\"span[class*=Tag--success]::text\").get()\n","            comapny = response.css(\n","                \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:first-child span::text\"\n","            ).get()\n","            location = (\n","                response.css(\n","                    \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:nth-child(2)\"\n","                )\n","                .re(r\">([^<]+)<\")[3]\n","                .strip()\n","            )\n","            yield response.follow(\n","                relative_url,\n","                callback=self.parse_inzerat,\n","                meta={\n","                    \"title\": title,\n","                    \"pay_range\": pay_range,\n","                    \"comapny\": comapny,\n","                    \"location\": location,\n","                },\n","            )\n","\n","        \"\"\"next_page = response.css(\n","            \"ul.Pagination li.Pagination__item:last-child a::attr(href)\"\n","        ).get()\n","        if next_page:\n","            next_page_url = self.domain_name + next_page\n","            yield response.follow(next_page_url, callback=self.parse)\"\"\"\n","\n","    def parse_inzerat(self, response):\n","        all_text = \"Not Found\"\n","        div_content = (\n","            response.css(\"section.cp-detail__content\")\n","            or response.css(\"div.cp-detail__content\")\n","            or response.css(\"div.content-rich-text\")\n","            or response.css(\"div.standalone.jobad__body\")\n","        )\n","        if div_content:\n","            # Extract text from <p> tags\n","            paragraphs = div_content.css(\"p::text\").getall()\n","\n","            # Extract text from <h2> tags\n","            headings = div_content.css(\"h2::text\").getall()\n","\n","            # Extract text from <ul> tags\n","            list_items = div_content.css(\"ul li::text\").getall()\n","\n","            # Extract text from <strong> tags\n","            strong_text = div_content.css(\"strong::text\").getall()\n","\n","            # Join all the extracted text into a single string\n","            all_text = \" \".join(\n","                paragraphs + headings + list_items + strong_text\n","            )\n","        else:\n","            #all_text = jobscz_selenium(response.url)\n","\n","        yield {\"url\": response.url, \"text\": all_text, **response.meta}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:01.351810Z","iopub.status.busy":"2023-06-22T12:58:01.351311Z","iopub.status.idle":"2023-06-22T12:58:01.368385Z","shell.execute_reply":"2023-06-22T12:58:01.367328Z","shell.execute_reply.started":"2023-06-22T12:58:01.351736Z"},"trusted":true},"outputs":[],"source":["class DevOpsSpider(scrapy.Spider):\n","    name = \"devops\"\n","    allowed_domains = [\"jobs.cz\"]\n","    start_urls = [\n","        \"https://www.jobs.cz/prace/?q%5B%5D=devops&q%5B%5D=dataops&q%5B%5D=mlops\"\n","    ]\n","            \n","    def parse(self, response):\n","        jobs = response.css(\"article.SearchResultCard\")\n","\n","        for job in jobs:\n","            relative_url = job.css(\"h2 a ::attr(href)\").get()\n","            title = job.css(\"h2 a ::text\").get().strip()\n","            pay_range = job.css(\"span[class*=Tag--success]::text\").get()\n","            comapny = response.css(\n","                \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:first-child span::text\"\n","            ).get()\n","            location = (\n","                response.css(\n","                    \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:nth-child(2)\"\n","                )\n","                .re(r\">([^<]+)<\")[3]\n","                .strip()\n","            )\n","            yield response.follow(\n","                relative_url,\n","                callback=self.parse_inzerat,\n","                meta={\n","                    \"title\": title,\n","                    \"pay_range\": pay_range,\n","                    \"comapny\": comapny,\n","                    \"location\": location,\n","                },\n","            )\n","\n","        \"\"\"next_page = response.css(\n","            \"ul.Pagination li.Pagination__item:last-child a::attr(href)\"\n","        ).get()\n","        if next_page:\n","            next_page_url = self.domain_name + next_page\n","            yield response.follow(next_page_url, callback=self.parse)\"\"\"\n","\n","    def parse_inzerat(self, response):\n","        all_text = \"Not Found\"\n","        div_content = (\n","            response.css(\"section.cp-detail__content\")\n","            or response.css(\"div.cp-detail__content\")\n","            or response.css(\"div.content-rich-text\")\n","            or response.css(\"div.standalone.jobad__body\")\n","        )\n","        if div_content:\n","            # Extract text from <p> tags\n","            paragraphs = div_content.css(\"p::text\").getall()\n","\n","            # Extract text from <h2> tags\n","            headings = div_content.css(\"h2::text\").getall()\n","\n","            # Extract text from <ul> tags\n","            list_items = div_content.css(\"ul li::text\").getall()\n","\n","            # Extract text from <strong> tags\n","            strong_text = div_content.css(\"strong::text\").getall()\n","\n","            # Join all the extracted text into a single string\n","            all_text = \" \".join(\n","                paragraphs + headings + list_items + strong_text\n","            )\n","        else:\n","            #all_text = jobscz_selenium(response.url)\n","\n","        yield {\"url\": response.url, \"text\": all_text, **response.meta, }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:01.370568Z","iopub.status.busy":"2023-06-22T12:58:01.369886Z","iopub.status.idle":"2023-06-22T12:58:01.388547Z","shell.execute_reply":"2023-06-22T12:58:01.387399Z","shell.execute_reply.started":"2023-06-22T12:58:01.370533Z"},"trusted":true},"outputs":[],"source":["class CloudSpider(scrapy.Spider):\n","    name = \"cloud\"\n","    allowed_domains = [\"jobs.cz\"]\n","    start_urls = [\n","        \"https://www.jobs.cz/prace/?q%5B%5D=cloud&q%5B%5D=cloudu\"\n","    ]\n","            \n","    def parse(self, response):\n","        jobs = response.css(\"article.SearchResultCard\")\n","\n","        for job in jobs:\n","            relative_url = job.css(\"h2 a ::attr(href)\").get()\n","            title = job.css(\"h2 a ::text\").get().strip()\n","            pay_range = job.css(\"span[class*=Tag--success]::text\").get()\n","            comapny = response.css(\n","                \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:first-child span::text\"\n","            ).get()\n","            location = (\n","                response.css(\n","                    \"ul.SearchResultCard__footerList li.SearchResultCard__footerItem:nth-child(2)\"\n","                )\n","                .re(r\">([^<]+)<\")[3]\n","                .strip()\n","            )\n","            yield response.follow(\n","                relative_url,\n","                callback=self.parse_inzerat,\n","                meta={\n","                    \"title\": title,\n","                    \"pay_range\": pay_range,\n","                    \"comapny\": comapny,\n","                    \"location\": location,\n","                },\n","            )\n","\n","        \"\"\"next_page = response.css(\n","            \"ul.Pagination li.Pagination__item:last-child a::attr(href)\"\n","        ).get()\n","        if next_page:\n","            next_page_url = self.domain_name + next_page\n","            yield response.follow(next_page_url, callback=self.parse)\"\"\"\n","\n","    def parse_inzerat(self, response):\n","        all_text = \"Not Found\"\n","        div_content = (\n","            response.css(\"section.cp-detail__content\")\n","            or response.css(\"div.cp-detail__content\")\n","            or response.css(\"div.content-rich-text\")\n","            or response.css(\"div.standalone.jobad__body\")\n","        )\n","        if div_content:\n","            # Extract text from <p> tags\n","            paragraphs = div_content.css(\"p::text\").getall()\n","\n","            # Extract text from <h2> tags\n","            headings = div_content.css(\"h2::text\").getall()\n","\n","            # Extract text from <ul> tags\n","            list_items = div_content.css(\"ul li::text\").getall()\n","\n","            # Extract text from <strong> tags\n","            strong_text = div_content.css(\"strong::text\").getall()\n","\n","            # Join all the extracted text into a single string\n","            all_text = \" \".join(\n","                paragraphs + headings + list_items + strong_text\n","            )\n","        else:\n","            #all_text = jobscz_selenium(response.url)\n","\n","        yield {\"url\": response.url, \"text\": all_text, **response.meta}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:58:01.391850Z","iopub.status.busy":"2023-06-22T12:58:01.390560Z","iopub.status.idle":"2023-06-22T12:58:02.539175Z","shell.execute_reply":"2023-06-22T12:58:02.537585Z","shell.execute_reply.started":"2023-06-22T12:58:01.391767Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T12:59:09.198497Z","iopub.status.busy":"2023-06-22T12:59:09.198047Z","iopub.status.idle":"2023-06-22T13:08:43.622499Z","shell.execute_reply":"2023-06-22T13:08:43.620781Z","shell.execute_reply.started":"2023-06-22T12:59:09.198463Z"},"trusted":true},"outputs":[],"source":["from scrapy.crawler import CrawlerRunner\n","from twisted.internet import reactor, defer\n","\n","runner = CrawlerRunner(settings={\n","    \"FEEDS\": {\n","        \"/kaggle/working/jobs_cz.json\": {\"format\": \"json\"},\n","    },\n","})\n","\n","spiders = [DataSpider, WebDevSpider, DevOpsSpider, CloudSpider]  # Add more spiders as needed\n","\n","@defer.inlineCallbacks\n","def crawl():\n","    for spider in spiders:\n","        yield runner.crawl(spider)\n","    reactor.stop()\n","crawl()\n","reactor.run()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Scrape startupjobs.cz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-22T12:58:03.029599Z","iopub.status.idle":"2023-06-22T12:58:03.030340Z","shell.execute_reply":"2023-06-22T12:58:03.030103Z","shell.execute_reply.started":"2023-06-22T12:58:03.030064Z"},"trusted":true},"outputs":[],"source":["\"\"\"import json\n","n=1 \n","postings = []\n","description = []\n","\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--disable-gpu')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","chrome_options.add_argument(\"--window-size=1920,1080\")\n","\n","driver = webdriver.Chrome(\n","    options=chrome_options\n",")\n","\n","# Open the webpage\n","driver.get(\n","    \"https://www.startupjobs.cz/nabidky?superinput=data&lokalita=ChIJEVE_wDqUEkcRsLEUZg-vAAQ,ChIJi3lwCZyTC0cRkEAWZg-vAAQ&lokalita-vzdalene=1\"\n",")  # Replace with the actual URL\n","driver.save_screenshot(\"ss.png\")\n","while True:\n","    try:\n","        # Find the div element by its ID\n","        wait = WebDriverWait(driver, 30)  # Maximum wait time of 10 seconds\n","        div_element = wait.until(\n","            EC.visibility_of_element_located((By.ID, \"offers-list\"))\n","        )\n","\n","        wait = WebDriverWait(driver, 10) \n","        article = wait.until(\n","            EC.visibility_of_element_located((By.TAG_NAME, \"article\"))\n","        )\n","        # Find all the article elements inside the div\n","        article_elements = div_element.find_elements(By.TAG_NAME, \"article\")\n","\n","        \n","        article_urls = []\n","        # Iterate over each article and extract the text\n","        for article in article_elements:\n","            atext = article.text\n","            link_element = article.find_element(By.CSS_SELECTOR, \"a\")\n","            article_urls.append(link_element.get_attribute(\"href\"))\n","            postings.append(atext)\n","\n","        for url in article_urls:\n","            driver.get(url)\n","            wait = WebDriverWait(driver, 10) \n","            text_element = wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"fr-view\")))\n","            text = text_element.text\n","            description.append(text)\n","\n","        n+=1\n","        break\n","\n","        driver.get(f\"https://www.startupjobs.cz/nabidky/strana-{str(n)}?superinput=data&lokalita=ChIJEVE_wDqUEkcRsLEUZg-vAAQ,ChIJi3lwCZyTC0cRkEAWZg-vAAQ&lokalita-vzdalene=1\")\n","    except Exception as e:\n","        print(e)\n","        break\n","\n","\n","driver.quit()\n","\n","data = {\n","    \"postings\": postings,\n","    \"descriptions\": description\n","}\n","with open('startup_jobs.json', 'w') as fp:\n","    json.dump(data, fp)\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T13:24:11.624097Z","iopub.status.busy":"2023-06-22T13:24:11.623629Z","iopub.status.idle":"2023-06-22T13:24:11.642832Z","shell.execute_reply":"2023-06-22T13:24:11.640975Z","shell.execute_reply.started":"2023-06-22T13:24:11.624061Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","def process_json_file(file_path):\n","    with open(file_path, 'r') as file:\n","        # Read the JSON file as a string\n","        json_string = file.read()\n","\n","    # Remove combinations: \"\\n][\\n\", \"]\\n\", and \"[\\n\"\n","    modified_string = json_string.replace('\\n][\\n', ',\\n')\n","\n","    # Load the modified string as a dictionary\n","    data = json.loads(modified_string)\n","\n","    return data\n","\n","# Example usage\n","file_path = '/kaggle/working/jobs_cz.json'\n","json_data = process_json_file(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T13:24:29.668238Z","iopub.status.busy":"2023-06-22T13:24:29.667789Z","iopub.status.idle":"2023-06-22T13:24:29.781121Z","shell.execute_reply":"2023-06-22T13:24:29.779469Z","shell.execute_reply.started":"2023-06-22T13:24:29.668204Z"},"trusted":true},"outputs":[],"source":["import json\n","import re\n","from statistics import median\n","\n","with open(\"/kaggle/input/test-scrape/devops_technologies.json\", \"r\", encoding=\"utf-8\") as f:\n","    devops = json.load(f)\n","with open(\"/kaggle/input/test-scrape/data_technologies.json\", \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","with open(\"/kaggle/input/test-scrape/web_technologies.json\", \"r\", encoding=\"utf-8\") as f:\n","    web = json.load(f)\n","\n","\n","def extract_pay(pay_range):\n","    cleaned_range = re.sub(r\"[^\\d\\s]\", \"\", pay_range)\n","\n","    values = cleaned_range.split()\n","\n","    int_values = [int(val) for val in values if int(val) != 0]\n","\n","    median_value = median(int_values)\n","\n","    return int(median_value * 1000)\n","\n","\n","def get_bigrams(text):\n","    # Split the text into individual words\n","    words = text.split()\n","\n","    # Create a list to store the bigrams\n","    bigrams = []\n","\n","    # Iterate through the words to generate bigrams\n","    for i in range(len(words) - 1):\n","        # Concatenate adjacent words to form a bigram\n","        bigram = (words[i], words[i + 1])\n","\n","        # Append the bigram to the list\n","        bigrams.append(bigram)\n","\n","    return bigrams\n","\n","\n","def keep_alphanumeric_space(text):\n","    # Use regex to keep only words, digits, and spaces\n","    cleaned_text = re.sub(r\"[^\\w\\s\\d.]\", \"\", text)\n","    return cleaned_text\n","\n","\n","ultra_dict = {**data, **devops, **web}\n","tech = ultra_dict.keys()\n","new_dict = []\n","for idx, i in enumerate(json_data):\n","    i[\"tech\"] = []\n","    txt_low = (\n","        i[\"text\"].lower() + \" \" + keep_alphanumeric_space(i[\"title\"].lower())\n","    )\n","    unigrams = txt_low.split(\" \")\n","    bigrams = get_bigrams(txt_low)\n","    for q in unigrams:\n","        if q in tech and q not in i[\"tech\"]:\n","            i[\"tech\"] += [q]\n","    for q in bigrams:\n","        if q in tech and q not in i[\"tech\"]:\n","            i[\"tech\"] += [q]\n","    new_data = {\n","        \"TITLE\": i[\"title\"],\n","        \"COMAPNY\": i[\"comapny\"],\n","        \"LOCATION\": i[\"location\"],\n","        \"PAY\": extract_pay(i[\"pay_range\"]) if i[\"pay_range\"] else \"Unknown\",\n","        \"REQUIRED_TECH\": i[\"tech\"],\n","        \"BASE_TEXT\": i[\"text\"],\n","        \"URL\": i[\"url\"],\n","    }\n","    new_dict.append(new_data)\n","with open(\"jobs_cz_docs.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(new_dict, f, ensure_ascii=False, indent=4)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
